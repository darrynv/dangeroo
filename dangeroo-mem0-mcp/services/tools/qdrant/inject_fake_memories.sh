#!/bin/bash

API_URL="http://localhost:8888/memories"
DEBUG=true

# Function to print debug messages
debug_log() {
  if [ "$DEBUG" = true ]; then
    echo "[DEBUG] $1"
  fi
}

# Function to process response
process_response() {
  local response=$1
  local memory_num=$2
  
  debug_log "Response for Memory #$memory_num:"
  # Extract status code (last line added by -w)
  local status_code=$(echo "$response" | tail -n 1)
  debug_log "Status Code: $status_code"
  
  # Extract JSON body (everything except the last line)
  local json_body=$(echo "$response" | sed '$d')
  
  # Check if response contains error or status code is not 2xx
  if echo "$json_body" | grep -q "error\|Error\|ERROR" || [[ ! "$status_code" =~ ^2[0-9]{2}$ ]]; then
    echo "[ERROR] Memory #$memory_num failed (Status: $status_code):"
    echo "$json_body" | jq . 2>/dev/null || echo "$json_body"
  else
    echo "[SUCCESS] Memory #$memory_num added (Status: $status_code)"
    debug_log "Response body: $(echo "$json_body" | jq . 2>/dev/null || echo "$json_body")"
  fi
  echo "----------------------------------------"
}

echo "Starting memory ingestion test with 10 programming examples..."
echo "API URL: $API_URL"
echo "----------------------------------------"

# Memory Example 1 - Python List Comprehension
debug_log "Posting Memory Example #1 (Python List Comprehension)"
response=$(curl -s -w "\n%{http_code}" -X POST "${API_URL}" \
  -H "Content-Type: application/json" \
  -d '{
  "messages": [
    {
      "role": "system",
      "content": "Python coding assistance session."
    },
    {
      "role": "user",
      "content": "How can I create a list of squares for numbers 0 through 9 in Python concisely?"
    },
    {
      "role": "assistant",
      "content": "You can use a list comprehension for that. It'\''s a very Pythonic way to create lists. The syntax would be `squares = [x**2 for x in range(10)]`. This iterates through numbers 0 to 9 (generated by `range(10)`) and squares each number `x`, adding the result to the new list `squares`."
    },
    {
      "role": "user",
      "content": "Ah, that'\''s much cleaner than a traditional for loop. Can I add a condition, like only squaring even numbers?"
    },
    {
      "role": "assistant",
      "content": "Absolutely! You just add an `if` condition at the end: `even_squares = [x**2 for x in range(10) if x % 2 == 0]`. This will only include the square if `x` is even."
    }
  ],
  "user_id": "python_dev_01",
  "agent_id": "coding_assistant_py",
  "run_id": "python_session_lc_20250424_01",
  "metadata": {
    "language": "Python",
    "topic": "Core Language Features",
    "sub_topic": "List Comprehensions",
    "concepts": ["iteration", "list creation", "conditional logic", "syntax"],
    "difficulty": "Beginner",
    "task_type": "code_generation_explanation",
    "code_snippet_provided": true,
    "libraries": ["built-in"]
  }
}')
process_response "$response" 1

# Memory Example 2 - OpenAI API Call (Completions)
debug_log "Posting Memory Example #2 (OpenAI API Call)"
response=$(curl -s -w "\n%{http_code}" -X POST "${API_URL}" \
  -H "Content-Type: application/json" \
  -d '{
  "messages": [
    {
      "role": "system",
      "content": "Assisting with OpenAI API integration in Python."
    },
    {
      "role": "user",
      "content": "I need to call the OpenAI completions endpoint using the `openai` Python library. How do I set up the client and make a basic chat completion request for gpt-4o?"
    },
    {
      "role": "assistant",
      "content": "Okay, first ensure you have the library installed (`pip install openai`). Then, you'\''ll instantiate the client, typically using your API key from environment variables. Here'\''s a basic example:\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\nresponse = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n  ]\n)\n\nprint(response.choices[0].message.content)\n```\nRemember to handle potential errors and manage your API key securely."
    },
     {
      "role": "user",
      "content": "Thanks! How do I access the response content?"
    },
    {
      "role": "assistant",
      "content": "The main response content is usually found in `response.choices[0].message.content`. The `response` object itself contains other useful information like usage statistics."
    }
  ],
  "user_id": "ai_integrator_02",
  "agent_id": "coding_assistant_py",
  "run_id": "openai_api_session_20250424_02",
  "metadata": {
    "language": "Python",
    "topic": "API Integration",
    "sub_topic": "OpenAI API",
    "concepts": ["API client", "authentication", "chat completions", "request structure", "response parsing", "environment variables"],
    "difficulty": "Intermediate",
    "task_type": "code_example_usage",
    "libraries": ["openai", "os"],
    "external_service": "OpenAI"
  }
}')
process_response "$response" 2

# Memory Example 3 - Semantic Search Concept
debug_log "Posting Memory Example #3 (Semantic Search Concept)"
response=$(curl -s -w "\n%{http_code}" -X POST "${API_URL}" \
  -H "Content-Type: application/json" \
  -d '{
  "messages": [
    {
      "role": "system",
      "content": "Explaining core AI and search concepts."
    },
    {
      "role": "user",
      "content": "What'\''s the difference between keyword search and semantic search?"
    },
    {
      "role": "assistant",
      "content": "Keyword search matches exact words or phrases in your query against documents. It'\''s literal. Semantic search, on the other hand, tries to understand the *meaning* and *intent* behind your query. It uses techniques like vector embeddings (often generated by models like those from OpenAI) to find documents that are conceptually similar, even if they don'\''t use the exact same keywords. For example, searching for \"best dog breeds for apartments\" semantically might return results about \"small quiet canines suitable for city living\"."
    },
    {
      "role": "user",
      "content": "So semantic search uses embeddings? How are those generated?"
    },
    {
      "role": "assistant",
      "content": "Yes, typically. Embeddings are numerical representations (vectors) of text, images, or other data, generated by deep learning models. Models like OpenAI'\''s `text-embedding-3-small` are trained to map semantically similar pieces of text to points that are close together in a high-dimensional vector space. This allows us to find related content by looking for vectors near our query vector."
    }
  ],
  "user_id": "ai_researcher_03",
  "agent_id": "ai_concepts_explainer",
  "run_id": "search_concepts_session_20250424_03",
  "metadata": {
    "language": "Conceptual",
    "topic": "Search Technology",
    "sub_topic": "Semantic Search vs Keyword Search",
    "concepts": ["search algorithms", "meaning", "intent", "vector embeddings", "vector space", "similarity", "natural language processing"],
    "difficulty": "Intermediate",
    "task_type": "concept_explanation",
    "related_technologies": ["Vector Databases", "OpenAI Embeddings", "NLP Models"]
  }
}')
process_response "$response" 3

# Memory Example 4 - Python Decorators
debug_log "Posting Memory Example #4 (Python Decorators)"
response=$(curl -s -w "\n%{http_code}" -X POST "${API_URL}" \
  -H "Content-Type: application/json" \
  -d '{
  "messages": [
    {
      "role": "system",
      "content": "Explaining advanced Python features."
    },
    {
      "role": "user",
      "content": "I keep seeing `@some_name` above function definitions in Python code. What does that syntax mean?"
    },
    {
      "role": "assistant",
      "content": "That'\''s a decorator! Decorators are a powerful Python feature that allow you to modify or enhance functions or methods in a clean, readable way. The `@decorator_name` syntax is syntactic sugar for applying the `decorator_name` function to the function defined below it. Essentially, `my_decorator(my_function)` becomes `@my_decorator` on the line before `def my_function():`. They are often used for logging, access control, instrumentation, and more."
    },
    {
      "role": "user",
      "content": "Could you show a simple example, maybe for timing a function?"
    },
    {
      "role": "assistant",
      "content": "Sure! Here'\''s a basic timing decorator:\n```python\nimport time\nimport functools\n\ndef timer(func):\n    @functools.wraps(func) # Preserves original function metadata\n    def wrapper_timer(*args, **kwargs):\n        start_time = time.perf_counter()\n        value = func(*args, **kwargs)\n        end_time = time.perf_counter()\n        run_time = end_time - start_time\n        print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\n        return value\n    return wrapper_timer\n\n@timer\ndef waste_some_time(num_times):\n    for _ in range(num_times):\n        sum([i**2 for i in range(1000)])\n\nwaste_some_time(100)\n```\nNow, whenever `waste_some_time` is called, the `timer` decorator will automatically wrap it, print the execution time, and return the original result."
    }
  ],
  "user_id": "python_dev_04",
  "agent_id": "coding_assistant_py",
  "run_id": "python_session_decorators_20250424_04",
  "metadata": {
    "language": "Python",
    "topic": "Advanced Language Features",
    "sub_topic": "Decorators",
    "concepts": ["metaprogramming", "higher-order functions", "closures", "function wrapping", "functools.wraps", "syntactic sugar"],
    "difficulty": "Intermediate",
    "task_type": "concept_explanation_code_example",
    "libraries": ["time", "functools"]
  }
}')
process_response "$response" 4

# Memory Example 5 - OpenAI Embeddings API Call
debug_log "Posting Memory Example #5 (OpenAI Embeddings API)"
response=$(curl -s -w "\n%{http_code}" -X POST "${API_URL}" \
  -H "Content-Type: application/json" \
  -d '{
  "messages": [
    {
      "role": "system",
      "content": "Assisting with OpenAI Embeddings API."
    },
    {
      "role": "user",
      "content": "How do I get vector embeddings for a list of text strings using the OpenAI Python library and the `text-embedding-3-small` model?"
    },
    {
      "role": "assistant",
      "content": "You use the `client.embeddings.create` method. Pass your list of strings to the `input` parameter and specify the model. Here'\''s how:\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\ntexts = [\"This is the first document.\", \"This is the second one.\"]\n\nresponse = client.embeddings.create(\n    input=texts,\n    model=\"text-embedding-3-small\"\n)\n\n# Access embeddings for each text\nfor i, embedding_data in enumerate(response.data):\n    print(f\"Embedding for text {i+1} (first 5 dims): {embedding_data.embedding[:5]}\")\n    # The full embedding is in embedding_data.embedding\n```\nThe response object'\''s `data` attribute will contain a list of objects, each with an `embedding` attribute holding the vector for the corresponding input text."
    }
  ],
  "user_id": "ai_integrator_05",
  "agent_id": "coding_assistant_py",
  "run_id": "openai_embeddings_session_20250424_05",
  "metadata": {
    "language": "Python",
    "topic": "API Integration",
    "sub_topic": "OpenAI Embeddings API",
    "concepts": ["vector embeddings", "API client", "text representation", "batch processing", "response structure"],
    "difficulty": "Intermediate",
    "task_type": "code_example_usage",
    "libraries": ["openai", "os"],
    "external_service": "OpenAI"
  }
}')
process_response "$response" 5

# Memory Example 6 - Vector Database Use Case (Qdrant)
debug_log "Posting Memory Example #6 (Vector DB Use Case)"
response=$(curl -s -w "\n%{http_code}" -X POST "${API_URL}" \
  -H "Content-Type: application/json" \
  -d '{
  "messages": [
    {
      "role": "system",
      "content": "Discussing vector database applications."
    },
    {
      "role": "user",
      "content": "We have a large knowledge base of technical articles. How could a vector database like Qdrant help us implement semantic search over these articles?"
    },
    {
      "role": "assistant",
      "content": "Qdrant is excellent for that. The process would be:\n1.  **Embedding Generation:** Convert each article (or relevant chunks of it) into a vector embedding using a model like OpenAI'\''s.\n2.  **Indexing:** Store these vectors in a Qdrant collection. You can also store metadata alongside each vector, like the article ID, title, author, publication date, and keywords.\n3.  **Querying:** When a user searches, convert their query into an embedding using the same model.\n4.  **Similarity Search:** Use Qdrant'\''s search capabilities to find the vectors in the collection that are closest (most similar) to the query vector.\n5.  **Retrieval:** Retrieve the metadata (like the article ID) associated with the top matching vectors and present the corresponding articles to the user.\nQdrant also allows filtering based on the stored metadata during the search, which can further refine results (e.g., find articles similar to the query published after a certain date)."
    }
  ],
  "user_id": "system_architect_06",
  "agent_id": "ai_concepts_explainer",
  "run_id": "vector_db_usecase_session_20250424_06",
  "metadata": {
    "language": "Conceptual",
    "topic": "Vector Databases",
    "sub_topic": "Semantic Search Implementation",
    "concepts": ["vector embeddings", "indexing", "similarity search", "metadata filtering", "knowledge base", "information retrieval"],
    "difficulty": "Intermediate",
    "task_type": "use_case_explanation",
    "related_technologies": ["Qdrant", "OpenAI Embeddings", "FastAPI", "Python"]
  }
}')
process_response "$response" 6

# Memory Example 7 - Python Asyncio Basics
debug_log "Posting Memory Example #7 (Python Asyncio)"
response=$(curl -s -w "\n%{http_code}" -X POST "${API_URL}" \
  -H "Content-Type: application/json" \
  -d '{
  "messages": [
    {
      "role": "system",
      "content": "Explaining Python'\''s asyncio library."
    },
    {
      "role": "user",
      "content": "I need to make multiple network requests concurrently in Python without using threads. I heard `asyncio` can help?"
    },
    {
      "role": "assistant",
      "content": "Yes, `asyncio` is Python'\''s standard library for writing concurrent code using the `async`/`await` syntax. It'\''s ideal for I/O-bound tasks like network requests or file operations. You define coroutines using `async def` and use `await` to pause execution when waiting for an operation (like a network call) to complete, allowing other tasks to run. You'\''d typically use a library like `aiohttp` for making the actual asynchronous HTTP requests."
    },
    {
      "role": "user",
      "content": "How do I run multiple `async` functions concurrently?"
    },
    {
      "role": "assistant",
      "content": "You can use `asyncio.gather()`. You create a list of awaitable objects (your coroutine calls) and pass them to `asyncio.gather()`. It will run them concurrently and return a list of their results once all have completed. Here'\''s a conceptual example:\n```python\nimport asyncio\nimport aiohttp\n\nasync def fetch(session, url):\n    async with session.get(url) as response:\n        return await response.text()\n\nasync def main():\n    urls = [\"url1\", \"url2\", \"url3\"]\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch(session, url) for url in urls]\n        htmls = await asyncio.gather(*tasks)\n        # Process htmls\n\nasyncio.run(main())\n```"
    }
  ],
  "user_id": "python_dev_07",
  "agent_id": "coding_assistant_py",
  "run_id": "python_asyncio_session_20250424_07",
  "metadata": {
    "language": "Python",
    "topic": "Concurrency",
    "sub_topic": "Asyncio",
    "concepts": ["asynchronous programming", "coroutines", "async/await", "event loop", "concurrency", "I/O bound", "asyncio.gather", "aiohttp"],
    "difficulty": "Intermediate",
    "task_type": "concept_explanation_code_example",
    "libraries": ["asyncio", "aiohttp"]
  }
}')
process_response "$response" 7

# Memory Example 8 - Fine-tuning OpenAI Models (Concept)
debug_log "Posting Memory Example #8 (OpenAI Fine-tuning Concept)"
response=$(curl -s -w "\n%{http_code}" -X POST "${API_URL}" \
  -H "Content-Type: application/json" \
  -d '{
  "messages": [
    {
      "role": "system",
      "content": "Discussing advanced OpenAI concepts."
    },
    {
      "role": "user",
      "content": "When should I consider fine-tuning an OpenAI model versus just using prompt engineering with a base model like gpt-4o?"
    },
    {
      "role": "assistant",
      "content": "Fine-tuning is beneficial when you need the model to consistently adopt a specific style, tone, or format, or to learn specialized knowledge not present in the base model, especially if this requires more examples than fit comfortably in a prompt. It'\''s essentially teaching the model a new skill or adapting its existing skills to your specific domain using a dataset of examples.\n\nPrompt engineering is often sufficient and more cost-effective for tasks where the base model already has the necessary knowledge and you just need to guide its output for a specific interaction. Start with prompt engineering; consider fine-tuning if:\n1. You need higher quality results than prompting alone provides.\n2. You want the model to consistently follow complex instructions or output formats.\n3. You need to adapt the model to information not well-represented in its original training.\n4. You have a large dataset (at least dozens, preferably hundreds or thousands) of high-quality examples of the desired input/output behavior."
    }
  ],
  "user_id": "ai_strategist_08",
  "agent_id": "ai_concepts_explainer",
  "run_id": "openai_finetuning_session_20250424_08",
  "metadata": {
    "language": "Conceptual",
    "topic": "Large Language Models",
    "sub_topic": "Fine-tuning vs Prompt Engineering",
    "concepts": ["model adaptation", "transfer learning", "prompting", "few-shot learning", "zero-shot learning", "style transfer", "knowledge injection", "training data"],
    "difficulty": "Advanced",
    "task_type": "strategy_comparison",
    "related_technologies": ["OpenAI API", "GPT models", "Machine Learning"]
  }
}')
process_response "$response" 8

# Memory Example 9 - Cosine Similarity for Semantic Search
debug_log "Posting Memory Example #9 (Cosine Similarity)"
response=$(curl -s -w "\n%{http_code}" -X POST "${API_URL}" \
  -H "Content-Type: application/json" \
  -d '{
  "messages": [
    {
      "role": "system",
      "content": "Explaining mathematical concepts behind semantic search."
    },
    {
      "role": "user",
      "content": "When doing semantic search with vector embeddings, how is the 'similarity' between the query vector and the document vectors actually calculated?"
    },
    {
      "role": "assistant",
      "content": "A very common metric used is **Cosine Similarity**. Instead of measuring the Euclidean distance between the vector endpoints, cosine similarity measures the cosine of the angle between the two vectors. It effectively measures orientation similarity, ignoring magnitude.\n\nVectors pointing in the same direction have a cosine similarity of 1. Orthogonal vectors have a similarity of 0. Vectors pointing in opposite directions have a similarity of -1.\n\nIn the context of embeddings (which are often normalized), a higher cosine similarity (closer to 1) indicates greater semantic similarity between the texts represented by the vectors. Vector databases like Qdrant are optimized to perform these calculations efficiently across millions or billions of vectors."
    }
  ],
  "user_id": "data_scientist_09",
  "agent_id": "ai_concepts_explainer",
  "run_id": "cosine_similarity_session_20250424_09",
  "metadata": {
    "language": "Conceptual",
    "topic": "Vector Math / Semantic Search",
    "sub_topic": "Cosine Similarity",
    "concepts": ["vector space", "similarity metrics", "angle between vectors", "dot product", "vector magnitude", "normalization", "vector databases"],
    "difficulty": "Intermediate",
    "task_type": "concept_explanation",
    "related_technologies": ["Linear Algebra", "Qdrant", "Embeddings"]
  }
}')
process_response "$response" 9

# Memory Example 10 - Python Error Handling (Try/Except)
debug_log "Posting Memory Example #10 (Python Error Handling)"
response=$(curl -s -w "\n%{http_code}" -X POST "${API_URL}" \
  -H "Content-Type: application/json" \
  -d '{
  "messages": [
    {
      "role": "system",
      "content": "Python error handling best practices."
    },
    {
      "role": "user",
      "content": "My Python script sometimes crashes if a file it tries to read doesn'\''t exist. How can I prevent the crash and handle this situation gracefully?"
    },
    {
      "role": "assistant",
      "content": "You should use a `try...except` block. Place the code that might cause an error (in this case, opening and reading the file) inside the `try` block. Then, add an `except` block specifically for the type of error you expect, like `FileNotFoundError`. If that error occurs within the `try` block, the code inside the `except` block will execute instead of crashing the program.\n```python\ntry:\n    with open(\"myfile.txt\", \"r\") as f:\n        content = f.read()\n    print(\"File read successfully.\")\nexcept FileNotFoundError:\n    print(\"Error: myfile.txt was not found.\")\nexcept Exception as e: # Catch other potential errors\n    print(f\"An unexpected error occurred: {e}\")\n```\nThis attempts to open the file. If it doesn'\''t exist, it prints a specific message. The final `except Exception` is a general catch-all for other unexpected issues during file processing."
    }
  ],
  "user_id": "python_dev_10",
  "agent_id": "coding_assistant_py",
  "run_id": "python_error_handling_session_20250424_10",
  "metadata": {
    "language": "Python",
    "topic": "Core Language Features",
    "sub_topic": "Error Handling",
    "concepts": ["exceptions", "try/except block", "FileNotFoundError", "graceful failure", "exception hierarchy", "file I/O"],
    "difficulty": "Beginner",
    "task_type": "code_example_best_practice",
    "libraries": ["built-in"]
  }
}')
process_response "$response" 10


# Summary Report
echo "Memory Ingestion Test Complete"
echo "----------------------------------------"
echo "Check logs above for individual memory results"
echo "To check if vectors were properly stored, you can run:"
echo "curl -X GET \"${API_URL}?user_id=python_dev_01\" | jq ."
echo "Or search by content:"
echo "curl -X POST \"${API_URL%/*}/search\" -H \"Content-Type: application/json\" -d '{\"query\": \"list comprehension\", \"user_id\": \"python_dev_01\"}' | jq ."